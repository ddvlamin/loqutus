{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"taxis\")\n",
    "         .config(\"spark.executor.cores\", \"4\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/yellow_tripdata_2019-01.csv\"\n",
    "datetime_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "QUERY_MONTH = \"2019-1\"\n",
    "columns_of_interest = [\n",
    "    \"VendorID\", \n",
    "    \"PULocationID\", \n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    \"payment_type\",\n",
    "    \"tpep_pickup_datetime\"\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(filepath)\n",
    "df = df.select(columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nytimezone = timezone(\"US/Eastern\")\n",
    "to_datetime = udf(lambda x: nytimezone.localize(datetime.strptime(x, datetime_format)))\n",
    "\n",
    "df = df.withColumn(\"tpep_pickup_datetime\", to_datetime(\"tpep_pickup_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_month(dt):\n",
    "    return f\"{dt.year}-{dt.month}\"\n",
    "\n",
    "year_month_str = udf(year_month)\n",
    "df = df.withColumn(\"year_month\", year_month_str(\"tpep_pickup_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_201901 = df.filter(df[\"year_month\"]==\"2019-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"taxi_drives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create aggregated trip fact table\n",
    "There are non-positive distances: we should include \"and trip_distance>0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_fact = spark.sql(\"\"\"\n",
    "    SELECT VendorID, PULocationID, payment_type, sum(fare_amount) as total_fare_amount, sum(trip_distance) as total_trip_distance \n",
    "    FROM taxi_drives \n",
    "    WHERE fare_amount > 0 \n",
    "    GROUP BY VendorID, PULocationID, payment_type\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_fact.createOrReplaceTempView(\"trip_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes distance per vendor and location for payment types 1 and 2\n",
    "vendor_pu_distance = spark.sql(\"\"\"\n",
    "    SELECT VendorID, PULocationID, sum(total_trip_distance) as vendor_pu_distance\n",
    "    FROM trip_fact\n",
    "    WHERE payment_type=1 or payment_type=2\n",
    "    GROUP BY VendorID, PULocationID\"\"\")\n",
    "vendor_pu_distance.createOrReplaceTempView(\"vendor_pu_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the above result with the original trip fact table\n",
    "trip_vendor_pu_distance = spark.sql(\"\"\"\n",
    "    SELECT tf.*, vpd.vendor_pu_distance\n",
    "    FROM trip_fact as tf\n",
    "    JOIN vendor_pu_distance as vpd\n",
    "    ON tf.VendorID=vpd.VendorID and tf.PULocationID=vpd.PULocationID\n",
    "    WHERE tf.payment_type=1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|VendorID|               tax|\n",
      "+--------+------------------+\n",
      "|       1| 77307.54199999999|\n",
      "|       4|1965.5500999999995|\n",
      "|       2| 135357.7713999999|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT VendorID, 0.01*sum(total_trip_distance) as tax\n",
    "    FROM trip_fact\n",
    "    GROUP BY VendorID\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 10000, 0.0), (10000, 30000, 0.1), (30000, 70000, 0.2), (70000, inf, 0.3)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentages = [\n",
    "    (0, 10000, 0.0),\n",
    "    (10000, 30000, 0.1),\n",
    "    (30000, 70000, 0.2),\n",
    "    (70000, np.inf,0.3)\n",
    "]\n",
    "percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tax_percentage(distance, percentages):\n",
    "    for lb, ub, percentage in percentages:\n",
    "        if lb <= distance < ub:\n",
    "            return percentage\n",
    "        \n",
    "add_tax_percentage_udf = udf(partial(add_tax_percentage, percentages=percentages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_tax_percentage = trip_vendor_pu_distance.withColumn(\"tax_percentage\", add_tax_percentage_udf(\"vendor_pu_distance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_tax_percentage.createOrReplaceTempView(\"trip_tax_percentage\")\n",
    "tax_df = spark.sql(\"\"\"\n",
    "    SELECT VendorID, sum(tax_percentage*total_fare_amount) as tax\n",
    "    FROM trip_tax_percentage\n",
    "    GROUP BY VendorID\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|VendorID|                 tax|\n",
      "+--------+--------------------+\n",
      "|       1|         7225624.523|\n",
      "|       4|           10879.253|\n",
      "|       2|1.2203507080999998E7|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tax_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressive_percentages(distance, percentages):\n",
    "    progressive_cut = 0\n",
    "    for lb, ub, percentage in percentages:\n",
    "        if distance > ub:\n",
    "            progressive_cut += percentage*(ub-lb)\n",
    "        else:\n",
    "            progressive_cut += percentage*(distance-lb)\n",
    "            break\n",
    "    try:    \n",
    "        return progressive_cut/distance\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "progressive_percentages_udf = udf(partial(progressive_percentages, percentages=percentages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert progressive_percentages(71000, percentages) == (20000*0.1+40000*0.2+0.3*1000)/71000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_tax_percentage = trip_vendor_pu_distance.withColumn(\"tax_percentage\", progressive_percentages_udf(\"vendor_pu_distance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|VendorID|                 tax|\n",
      "+--------+--------------------+\n",
      "|       1|    5534185.76709494|\n",
      "|       4|   6196.049828009542|\n",
      "|       2|1.0287641340610534E7|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_tax_percentage.createOrReplaceTempView(\"trip_tax_percentage\")\n",
    "tax_df = spark.sql(\"\"\"\n",
    "    SELECT VendorID, sum(tax_percentage*total_fare_amount) as tax\n",
    "    FROM trip_tax_percentage\n",
    "    GROUP BY VendorID\n",
    "\"\"\")\n",
    "tax_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
